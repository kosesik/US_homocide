---
title: "RML_project_US homocide"
author: "Krzysztof Osesik"
date: "15 06 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(LANGUAGE='en')
set.seed(123)
```

##

```{r, echo=FALSE}

# Setting up working directory
path_work<-"Z:/moje/moje/studia QPE/przedmioty/II semestr/Responsible ML/projekt/US homocide"
path_home<-"C:/Users/t420/Desktop/moje/studia QPE UW/przedmioty/II semestr/Responsible ML/projekt/US homocide"
path_home_2<-"C:/Users/Krzysztof_ASUS/Desktop/moje/studia QPE/przedmioty/II semestr/Responsible Machine Learning/project/US homocide"

location<-2

setwd(switch(location,path_work,path_home,path_home_2))

rm(list=ls())
```

# **1. Introduction**

## 1.1 Problem Context

The Murder Accountability Project is the most complete database of homicides in the United States currently available. This dataset includes murders from the FBI's Supplementary Homicide Report from 1976 to the present. 

Dataset comes from Kaggle (https://www.kaggle.com/murderaccountability/homicide-reports).

The dataset consists of nearly 640,000 crime cases. They are described by 24 variables, for example City, State, Year, Crime Type, Victim's Age, Perpetrator's Age and Weapon Used. 


## 1.2 Stakeholder

A stakeholder could be associated with a public authority aiming at ensuring that – in a case of a murder – identical effort is put into investigation with no regard for the victim’s race, sex or age. As a result similar crimes should be solved (alternatively not solved) independent of the victim’s race, sex  and age. 

The stakeholder is thus interested in evaluating the model’s general performance.  Their goal is to analyze whether there is some bias in police investigations towards certain social groups, which effectively results in a lower rate of solved crimes for those groups. 


## 1.3 Objectives

The goal of the analysis is to determine whether fairness of the model is maintained. To that end, I analyze whether identified protected variables affect the outcome of the crime investigation. In the dataset victim's age, race and sex were identified as protected variables and whether a crime was solved or not was identified as target variable. In addition, explainability analysis was also conducted.

To sum up, the following are the objectives of the project:

1) Exploratory Data Analysis
2) Construction of predictive model
3) Explainability analysis
4) Assessment of model's fairness


## 1.4 Data

The dataset consists of nearly 640,000 crime cases. They are described by 24 variables, for example City, State, Year, Crime Type, Victim's Age, Perpetrator's Age and Weapon Used. 

In addition, in order to increase computational efficiency, the dataset was reduced to look only at crimes in five U.S. States, i.e. Louisiana, Alabama, Mississippi, Georgia and South Carolina, which are known to be less friendly towards ethnic minorities (especially black). The timeframe of the analysis is 1980s (1980-1989).

Crime Solved was chosen as target variable. This variable defines whether a crime was solved or not. The remaining of 23 variables was chosen to be independent variables ( not all of which are relevant).

In the final model, the following variables were used:

*Target variable*

* Crime Solved 

The data is however imbalanced, as there is less than 30% of cases where crime remained unsolved. This will need to be dealt with in the process of the analysis.


*Explanatory variables*

1.	Agency Type (e.g. County Police, State Police, etc.)
2.	Victim’s Sex (Female, Male)
3.	Victim’s Race (e.g. Black, White, Native, etc.)
4.	Victim’s Age ( continuous 18-100 years old)
5.	Weapon (e.g. Knife, Gun, Poison, etc.)
6.	Victim’s Count ( how many additional victims there were – integral 0 – 10 people)

*Protected variables*

1.	Victim’s Sex
2.	Victim’s Race
3.	Victim’s Age 




<!-- Loading required libraries. -->

```{r, echo=FALSE,results="hide",, message=FALSE, warning=FALSE}
library(MASS)
library(tidyverse)
library(corrplot)
library(tidyselect)
library(scales)
library(dgof)
library(caTools)
library(pROC)
library(dummy)
library(smotefamily)
library(officer)
library(ROSE)
library(pscl)
library(caret)
library(OptimalCutpoints)
library(randomForest)
library(arm)
library(e1071)
library(DescTools)
library(reshape2)
library(lime)
library(DALEX)
library(vip)
library(pdp)
library(fairness)
library(DALEXtra)
library(fastshap)
```

<!-- Loading the data. -->

```{r, echo=FALSE,results="hide", message=FALSE, warning=FALSE}
# data<-read_csv(file = "database.csv")

# save(data,file="data.R")
load("data.R")
data_org<-data

```

The structure of the data is as follows. 

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}
dim(data)
# View(data)
summary(data)
```
```{r,echo=FALSE}
str(data)
```

<!-- Column names have spaces. Let's replace them with "_". -->


```{r, echo=FALSE, results="hide"}
temp<-colnames(data)

temp<-gsub(" ","_",temp)

colnames(data)<-temp


```

The dataset is for all U.S. states for years 1980-2014. However, we are only interested in a subset of that, namely in results for five states of Louisina,Alabama, Mississipi, Georgia and South Carolina (so-called US Deep South States) in the 1980s decade (1980-1989). These five states were known to be have been rather unfriendly to minorities, especially black minorities in those and previous years. 

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}
data<-subset(data, data$State %in% c("Louisiana","Alabama","Mississipi", "Georgia","South Carolina") & between(data$Year,1980, 1989))


```


<!-- Let's check if there are any empty values. -->


```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

temp<-sum(sapply(data,function(x) (length(x)==length(data$'Record_ID'))))-ncol(data)

if( temp==0){
  
  cat("There are no empty cells in the dataset and every column is equal in length.")
  
} else{
  
  cat("There are",temp,"column(s) in the dataset with empty cells.")
}
```


<!-- Now, let's check if there are any missing values. -->


```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

temp<-sum(sapply(data,function(x) (is.na(x))))

if( temp==0){
  
  cat("There are no missing values in the dataset")
  
} else{
  
  cat("There are",temp,"missing value(s) in the dataset.")
}
```
<!-- Since there is only 1 missing values in the dataset we can remove it from the data. -->

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

data<-data[complete.cases(data),]

```

<!-- Let's remove variables which will not be used in the analysis. -->

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}
temp<-c("Record_ID" ,"Agency_Code","Agency_Name", "City", "State", "Year","Month","Incident","Record_Source", "Victim_Ethnicity", "Perpetrator_Ethnicity",
     "Crime_Type","Agency_Name","Perpetrator_Count","Perpetrator_Race","Perpetrator_Age","Perpetrator_Sex", "Relationship" )

data<-data[,-(which(colnames(data) %in% temp))]


```

In my analysis, I will only look at the cases with perpetrators over 18 years old and remove cases with Victim_age mistakenly assigned to 998.

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

data<-subset(data,Victim_Age>=18 & Victim_Age!=998)

```



<!-- Let's turn character variables into factor. -->


```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

data<-data %>% 
 mutate_if(is.character,as.factor) 

```

<!-- Let's attach the data. -->

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}
attach(data)
```


# **2. Exploratory Data Analysis**

Let's look at the histogram of only one continuous variable in the dataset (Victim's Age). However, later on for the purpose of ease of analysis, the single continuous variable was turned into categorical one.


```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

data %>% 
  select(Victim_Age, Crime_Solved) %>% 
  ggplot(aes(x=Victim_Age,fill=Crime_Solved))+geom_density(alpha=0.4)+theme_bw()


```


<!-- Let's group categorical variables into fewer levels. -->

<!-- # FEATURE ENGINEERING -->

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}


data<-subset(data,subset=Victim_Sex!="Unknown" & Victim_Race!="Unknown")


data$Victim_Sex<-droplevels(data$Victim_Sex,exclude = anyNA(levels(data$Victim_Sex)))
data$Victim_Race<-droplevels(data$Victim_Race,exclude = anyNA(levels(data$Victim_Race)))


detach(data)

data$Weapon<-as.character(data$Weapon)

temp<-c("Firearm","Gun","Handgun","Rifle","Shotgun")

data$Weapon[which(data$Weapon %in% temp)]<-"Gun"

temp<-c("Strangulation","Suffocation")

data$Weapon[which(data$Weapon %in% temp)]<-"Suffocation"

data$Weapon<-as.factor(data$Weapon)


attach(data)

data$Victim_Age<-cut(data$Victim_Age,breaks=c(18,30,45,60,75,100), labels = c("18-30","30-45","45-60","60-75","75+"),right = FALSE)


```


Let's look at some distributions of categorical variables.


```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

temp<-colnames(data)

for (i in (temp)){
  
p<-data %>% 
  select(as.name(i)) %>% 
  ggplot(aes(x=eval(parse(text=i)),y=..count../sum(..count..)))+geom_bar(fill="sky blue",position = "dodge")+
  labs(y="Frequency", x=i)+
  theme_bw()+
  geom_text(aes(label=round(..count../sum(..count..),3)*100,y=(..count../sum(..count..))),stat="count",vjust=-.1)+
    theme(axis.text.x = element_text(angle=90))

print(p)


}



```

In our reduced dataset around 80% of crimes were solved. Most victims were male (ca. 75%), black (ca. 65%) and below 45 years of age (ca. 75%). gun was the most common weapon used to commit a crime.



Now, let's take a look at conditional distributions for protected categorical variables, i.e. Victim_Sex,  Victim_Race and Victim_Age conditional on whether the crime was solved or not.

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}


  p<-data %>% 
  select(Crime_Solved,Victim_Sex) %>% 
  ggplot(aes(x=Victim_Sex, group=Crime_Solved, y=(..prop..), fill=factor(..x..)))+geom_bar(position ="dodge")+
    facet_grid(~Crime_Solved)+
    geom_text(aes(y=(..prop..),label=scales::percent(round(..prop..,3))),stat="count", vjust=-.5)+
    scale_y_continuous(labels=scales::percent, limits=c(0,0.9))+
    labs(y="Percent", fill=Victim_Sex)+
    scale_fill_discrete(labels = levels(Victim_Sex))+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 90))


print(p)


  p<-data %>%
  select(Crime_Solved,Victim_Age) %>%
  ggplot(aes(x=Victim_Age, group=Crime_Solved, y=(..prop..), fill=factor(..x..)))+geom_bar(position ="dodge")+
    facet_grid(~Crime_Solved)+
    geom_text(aes(y=(..prop..),label=scales::percent(round(..prop..,3))),stat="count", vjust=-.5)+
    scale_y_continuous(labels=scales::percent, limits=c(0,0.5))+
    labs(y="Percent", fill=Victim_Age)+
    scale_fill_discrete(labels = levels(Victim_Age))+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 90))+
    theme(legend.position = "none")


print(p)

  p<-data %>% 
  select(Crime_Solved,Victim_Race) %>% 
  ggplot(aes(x=Victim_Race, group=Crime_Solved, y=(..prop..), fill=factor(..x..)))+geom_bar(position ="dodge")+
    facet_grid(~Crime_Solved)+
    geom_text(aes(y=(..prop..),label=scales::percent(round(..prop..,3))),stat="count", vjust=-.5)+
    scale_y_continuous(labels=scales::percent, limits=c(0,0.9))+
    labs(y="Percent", fill=Victim_Race)+
    scale_fill_discrete(labels = levels(Victim_Race))+
    theme_bw()+
    theme(axis.text.x = element_text(angle = 90))
   
   print(p)

```

From the first look at conditional distributions of protected variables there is no clear pattern of biases visible for victim's sex and victim's race. The exception is the victim's age variable. It seems like the younger the victim was, the higher was a probability that the crime would be solved. Older victims , i.e. people over 75 years of age constituted nearly 8% of unsolved cases, but only above 3% of solved cases.


Let's take a look at correlation (association) between categorical variables in the dataset. I will use Thiel's uncertainty coefficient to that.

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

cor_df<-matrix(0,nrow=ncol(data), ncol=ncol(data), dimnames=list(colnames(data),colnames(data)))

for (i in 1:ncol(data)){
  
  for (j in 1:ncol(data)){
    
    temp<-UncertCoef(table(data[[i]],data[[j]]), direction = "column")
    
    cor_df[i,j]<-temp
    
  }
  
}

cor_df<-round(cor_df, 2)
cor_df<-melt(cor_df)

ggplot(cor_df,aes(x=Var1,y=Var2,fill=value))+geom_tile()+labs(title = "Uncertainity Coefficient for categorical variables")+geom_text(aes(Var1, Var2, label = value), color = "black", size = 4)+scale_fill_gradient(low="white", high="blue") 


```

The variables do not seem to be correlated.



# **3. MODELLING**


In the modelling part of the analysis I will use three algorithms: 

1. logistic regression
2. random forest 
3. support vector mechanism 


The algorithm that offers the best results will be the basis of further analysis in terms of explainability and fairness. 

Let's first divide the dataset into training and test sets.

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}
temp<-sample.split(data$Crime_Solved,SplitRatio = 0.7)

train_set<-subset(data, temp==TRUE)
test_set<-subset(data, temp==FALSE)

```

Let's see if proportions of Yes/No in target variable are maintained.

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

cat("The proportion of cases in the original dataset:")
round(prop.table(table(data$Crime_Solved)),4)*100
cat("\n ")
cat("The proportion of cases in the train set dataset:")
round(prop.table(table(train_set$Crime_Solved)),4)*100
cat("\n ")
cat("The proportion of cases in the test set dataset:")
round(prop.table(table(test_set$Crime_Solved)),4)*100

```

Proportions of binary responses of target variable have been maintained in the training and testing sets. Now, I need to deal with high imbalance of the training dataset. To that end, I will apply both oversampling and undersampling methods. This method was found to provide better results than oversampling and/or undersampling on a stand-alone basis.


```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

train_set_balanced<-ovun.sample(Crime_Solved~., data=train_set,method="both",p=0.5)

train_set_balanced<-train_set_balanced$data

table(train_set_balanced$Crime_Solved)


```


## 3.1 LOGISTIC REGRESSION

Let's start predictive modelling. First, logistic regression model will be applied. Let's look at its summary.

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}
train_set_balanced$Crime_Solved<-ifelse(train_set_balanced$Crime_Solved=="Yes",1,0)

train_set_balanced$Crime_Solved<-factor(train_set_balanced$Crime_Solved, levels=c(0,1))

model_log<-glm(Crime_Solved~.,data=train_set_balanced, family=binomial (link="logit"))

summary(model_log)

```

Some dummy variables are indicated as significant. Now, let's look at some other diagnostics.


```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}


# cat("McFadden R-squared for logistic regression is equal to:")
# pR2(model_log)["McFadden"]
# 
# cat("\n")
anova(model_log, test="Chisq")

# vars_imp<-varImp(model_log)
# 
# vars_type<-rownames(vars_imp)
# 
# vars_imp<-cbind(vars_type,vars_imp)
# 
# rownames(vars_imp)<-NULL
# 
# cat("\n")
# head(vars_imp[order(vars_imp$Overall,decreasing = TRUE),],10)



```


From the anova analysis, it looks like for the logistic regression model - Weapon, Agency Type and Victim's Age are the most important explanatory variables (largest deviances). 

Now, let's try predict values based on a test set data and evaluate confusion matrix and ROC curve.

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

test_set$Crime_Solved<-ifelse(test_set$Crime_Solved=="Yes",1,0)

pred_log<-predict(model_log, test_set[-2],type="response")
pred_log_bin<-ifelse(pred_log>=0.5,1,0)

confusionMatrix(as.factor(pred_log_bin),as.factor(test_set$Crime_Solved), positive = "0")


roc<-roc(test_set$Crime_Solved~pred_log,plot=TRUE,print.auc=TRUE,col="green",lwd =4,legacy.axes=TRUE,main="ROC Curve - Logistic Regression")


```

Let's look at the results of logistic regression modeling with a use of cross-validiation.

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
mod_log_cv <- train(Crime_Solved~.,  data=train_set_balanced, method="glm", family="binomial",
                 trControl = ctrl, tuneLength = 5)

pred_log_cv <- predict(mod_log_cv, newdata=test_set[-2], type="prob")

pred_log_cv_bin<-ifelse(pred_log>=0.5,1,0)

confusionMatrix(as.factor(pred_log_cv_bin),as.factor(test_set$Crime_Solved))


roc<-roc(test_set$Crime_Solved~pred_log_cv[,2],plot=TRUE,print.auc=TRUE,col="green",lwd =4,legacy.axes=TRUE,main="ROC Curve - Logistic Regression Cross Validation")

```

The results with or without the use of cross-validation are identical. 


## 3.2 RANDOM FOREST

Now, let's apply random forest algorithm using randomForest package. LEt's investigate confusion matrix and ROC curve.

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

model_rf<-randomForest(Crime_Solved~.,data=train_set_balanced, ntree=30 )

pred_rf<-predict(model_rf,newdata=test_set[-2],type="prob")

pred_rf_bin<-ifelse(pred_rf[,2]>0.5,1,0)

confusionMatrix(as.factor(pred_rf_bin),as.factor(test_set$Crime_Solved))

roc<-roc(test_set$Crime_Solved~pred_rf[,2],plot=TRUE,print.auc=TRUE,col="green",lwd =4,legacy.axes=TRUE,main="ROC Curve - Random Forest")


# model_rf

```

Let's look at some additional diagnostics for the random forest model.


```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

obb_error_df<-data.frame(
  Trees=rep(1:nrow(model_rf$err.rate), times=3),
  Type=rep(c("OOB","0","1"), each=nrow(model_rf$err.rate)),
  Error=c(model_rf$err.rate[,"OOB"],
          model_rf$err.rate[,"0"],
          model_rf$err.rate[,"1"]))


ggplot(data=obb_error_df, aes(x=Trees, y=Error))+geom_line(aes(color=Type))+theme_bw()+labs(main="Random forest out-of-bag error & number of trees")

oob_values<-vector(length=10)

for (i in 1:(ncol(data)-1)){
  
  temp_model_rf<-randomForest(Crime_Solved~.,data=train_set_balanced, ntree=30,mtry=i)
  
  oob_values[i]<-temp_model_rf$err.rate[nrow(temp_model_rf$err.rate),1]
  
}  
```

It looks like out-of-bag error rate stabilizes at around 30 trees.


```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

varImp<-importance(model_rf)

varImpPlot(model_rf)




```

It looks like the most important variable for the random forest algorithm was Weapon and then Agency_Type and Victim_Age.




## 3.3 SVM

Now, let us apply support vector mechanism (SVM) algorithm. I will run tunning mechanism in order to find optimal parameters for svm model.

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

set.seed(123)



model_svm_tuned<-tune(svm,Crime_Solved~.,data=train_set_balanced,
                      kernel="radial",
                      ranges=list(gamma=2^(0),cost=10^(0)))

cat("The optimal parameters for svm are:")
model_svm_tuned$best.parameters

```

<!-- Let's use the best parameters found in the tunning process to evaluate svm. -->



```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}


model_svm<-svm(Crime_Solved~.,data=train_set_balanced, kernel="radial",
               gamma=model_svm_tuned$best.parameters$gamma,
               cost=model_svm_tuned$best.parameters$cost,
               type="C-classification",
               scale=TRUE)


summary(model_svm)

```

Let's predict responses based on test set and evaluate confusion matrix.

```{r, echo=FALSE,results="markup", message=FALSE, warning=FALSE}

pred_svm<-predict(model_svm,newdata=test_set[-2])


confusionMatrix(as.factor(pred_svm),as.factor(test_set$Crime_Solved))


roc<-roc(test_set$Crime_Solved~as.numeric(as.character(pred_svm)),plot=TRUE,print.auc=TRUE,col="green",lwd =4,legacy.axes=TRUE,main="ROC Curve - SVM")

```

SVM performs very similarly to Random forest model. However, Random forest model, at the cost of slightly worse model accuracy (positive prediction value is the same) fares better in terms of sensitivity metrics, i.e. in identifying positive cases ( for the purpose of our analysis "positive case" is associated with crime not-solved - factor level "0"). Therefore, in the subsequent parts, i.e. explainability analysis and fairness analysis Random forest will be used.



# **4. EXPLAINABILITY  ANALYSIS**

In the explainability part I will focus on:

1.two global metrics:

  +     variable importance plots
  +     partial dependence plots
  
2. two local metrics:

  +     LIME
  +     Shapley Values



## 4.1 VARIABLE IMPORTANCE PLOTS

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

explain_rf <- DALEX::explain(model = model_rf,  
                        data = test_set[,-2],
                           y = test_set$Crime_Solved,
                        label="Random Forest")


loss_root_mean_square(observed = test_set$Crime_Solved,
                   predicted = predict(model_rf, test_set))
                   
```

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

set.seed(123)
vip_50<-model_parts(explainer = explain_rf, 
        loss_function = loss_root_mean_square,
                    B = 30,
        type="raw")


plot(vip_50)


```

Based on Variable Importance Plots it looks like Agency_Type and Weapon are the most important variables for determining whether a crime was solved or not. Their impact however is not very strong.  


## 4.2 PARTIAL DEPENDENCE PLOTS


```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

explain_rf <- DALEX::explain(model = model_rf,  
                        data = test_set[,-2],
                           y = test_set$Crime_Solved,
                        label="Random Forest")
```
```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

pdp_fun<-function(x){
  
  
  
  pdp_rf<-model_profile(explainer=explain_rf, variables=x,
                        # groups=,
                        )
  

}

col_n<-colnames(test_set)[-2]


for (i in seq_along(col_n)){
  
print(plot(pdp_fun(col_n[i])))
  
}



```

Partial dependence plots show that there are considerable differences in expected model predictions (average probabilities) as to whether a crime was solved or not. For example, in some Victim_Age categories average probability of solving a crime was much higher than in other (for age category 45-60 average prediction was three times higher than for 75+ category). Similarly, when looking at the second protected variable, i.e. Victim_Sex, average probability of solving a crime for female was nearly twice as high as for male. In the case, of the third protected variable, i.e. Victim_Race there does not seem to be a considerable difference between races. 

In addition, partial dependence plots confirm also what was already indicated by variable importance plots, i.e. that there are substantial differences between average probabilities of solving a crime depending on the type of agency running an investigation and the weapon used in a crime.

## 4.3 LIME

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

model_rf<-as_classifier(model_rf, labels = NULL)

test_set$Crime_Solved<-as.factor(test_set$Crime_Solved)
test_set$Victim_Count<-as.numeric(test_set$Victim_Count)


explainer <- lime(train_set_balanced[,-2], model_rf, quantile_bins = FALSE)

set.seed(123)

d_test<-subset(test_set,test_set$Victim_Race=="Black"& test_set$Victim_Age=="75+"&test_set$Weapon=="Gun"& test_set$Agency_Type=="Sheriff"&test_set$Victim_Sex=="Male")

explanation <- lime::explain(d_test[c(3:4,(nrow(d_test)-1):nrow(d_test)),-2], explainer,n_labels=1, n_features = 6, n_permutations=100)


# explanation


plot_features(explanation)


```

For the Lime analysis I have chosen four instances of the same combination of variables, i.e.:
1. person was black,
2. over 75 years old,
3. weapon was gun,
4. investigation was carried out by the Sheriif,
5. he was a male

It looks like the results are rather stable. For each instance the fact that the investigation was executed by the Sheriff increased probability of solving the crime ( in three instances it was the most important variable). On contrary, the fact that a victim was male, reduced the expected probability of solving the crime (second most important variable in three instances). 
The fact that a person was black had little influence on the expected probability. However, what is rather unexpected, given the results of variable importance plots and partial dependence plots is the fact that the persons were over 75 years old also had little impact on the expected probability.



## 4.4 SHAPLEY VALUES

Now, let's look at an alternative approach, i.e. so-called Shapley values.

```{r,echo=FALSE,results="hide", message=FALSE, warning=FALSE}

explain_rf <- DALEX::explain(model = model_rf,  
                        data = test_set[,-2],
                           y = test_set$Crime_Solved,
                        label="Random Forest")
predict(explain_rf, test_set[3:5,])



shap <- predict_parts(explainer = explain_rf, 
                      new_observation = d_test[3,], 
                                 type = "shap",
                                    B = 25)

shap

```
```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

plot(shap)

```

For the SHAP values analysis, I took Case 1 from LIME analysis in order to ensure that SHAP values analysis could be compared to LIME results. As evidenced by the plot, the fact that the Sheriff conducted this investigation had a positive impact on the probability of solving the crime. In opposition to LIME however, SHAP values identified much bigger influence of the victim's age category on the expected probability. The fact that the person was black increased his chances of solving the crime. The remaining 

# **5. FAIRNESS ANALYSIS**

Now, let's turn to fairness analysis. Here I will use three metrics:

1. Predictive rate parity
2. Proportional parity
3. Equalized odds

At the stage of explanatory model analysis, I identified three potential protected variables, i.e.:

1.	Victim’s Sex
2.	Victim’s Race
3.	Victim’s Age

However, in all models applied, and in the Random Forest in particular Victim's Race does not seem to be a significant variable, which indicates that racial profiling did not have much weight on crime investigations. In turn, out of the three protected variables, Victim's Age seems most significant, followed by Victim's Sex. Thus, in the subsequent fairness analysis I will only focus on these two protected variables.



## 5.1 PREDICTIVE RATE PARITY

Before looking at predictive rate parity, let's first look at the densities of predicted probabilities for different subgroups of Victim's Age and Victim's Sex in the dataset.

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

test_set_fair<-cbind(test_set,pred_rf)

colnames(test_set_fair)[(ncol(test_set_fair)-1):ncol(test_set_fair)]<-c("probs_0","probs_1")

prp_sex <- pred_rate_parity(data         = test_set_fair, 
                         outcome      = "Crime_Solved", 
                         outcome_base = '0', 
                         group        = 'Victim_Sex',
                         probs        = 'probs_1', 
                         cutoff       = 0.5, 
                         base         = 'Female')




prp_age <- pred_rate_parity(data         = test_set_fair, 
                         outcome      = "Crime_Solved", 
                         outcome_base = '0', 
                         group        = 'Victim_Age',
                         probs        = 'probs_1', 
                         cutoff       = 0.5, 
                         base         = '30-45')


prp_sex[3]
prp_age[3]

# "Formula: TP / (TP + FP) - precision"

```

The two plots confirm the results of previous analyses. If a victim was a female the crime was much more likely to be solved rather than when a victim was a male. By the same token, average probabilities of solving a crime were different for different victim's age subgroups, with especially older people being here at disadvantage.


Now, let's see predictive rate parity results.


```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}
prp_sex[2]
prp_age[2]


```

Predictive rate parity does not show considerable differences given victim's sex and victim's age. Precision of the model ( TP/ (TP +FP) in the confusion matrix ) is basically the same for all instances of these two variables.


<!-- ## DEMOGRAPHIC PARITY -->


<!-- ```{r} -->


<!-- dem_par_sex <- dem_parity(data     = test_set_fair,  -->
<!--                       outcome      = 'Crime_Solved', -->
<!--                       outcome_base = '0',  -->
<!--                       group        = 'Victim_Sex', -->
<!--                       probs        = 'probs_1', -->
<!--                       base         = "Female") -->
<!-- dem_par_sex[1:2] -->



<!-- dem_par_age <- dem_parity(data     = test_set_fair,  -->
<!--                       outcome      = 'Crime_Solved', -->
<!--                       outcome_base = '0',  -->
<!--                       group        = 'Victim_Age', -->
<!--                       probs        = 'probs_1', -->
<!--                       base         = "30-45") -->
<!-- dem_par_age[1:2] -->


<!-- "Formula: (TP + FP) - absolute number of all positively classified individuals in all subgroups" -->

<!-- ``` -->


## 5.2 PROPORTIONAL PARITY

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

prop_par_sex <- prop_parity(data     = test_set_fair, 
                      outcome      = 'Crime_Solved',
                      outcome_base = '0', 
                      group        = 'Victim_Sex',
                      probs        = 'probs_1',
                      base         = "Female")
prop_par_sex[2]


prop_par_age <- prop_parity(data     = test_set_fair, 
                      outcome      = 'Crime_Solved',
                      outcome_base = '0', 
                      group        = 'Victim_Age',
                      probs        = 'probs_1',
                      base         = "30-45")
prop_par_age[2]

# "Formula: (TP + FP) / (TP + FP + TN + FN) - proportion of all positively classified individuals in all subgroups"

```

Proportional parity plot shows the proportion of all positively classified individuals in a given subgroup (positively classified means in this case that a crime was expected to be solved). In the cases of my analysis, the plots show that male had much smaller chances of their crimes to be solved. The same was true for older people, i.e. in age group categories 60-75 and particulary 75+.

## 5.3 EQUALIZED ODDS

```{r,echo=FALSE,results="markup", message=FALSE, warning=FALSE}

eq_odds_sex <- equal_odds(data     = test_set_fair, 
                      outcome      = 'Crime_Solved',
                      outcome_base = '0', 
                      group        = 'Victim_Sex',
                      probs        = 'probs_1',
                      base         = "Female")
eq_odds_sex[2]



eq_odds_age <- equal_odds(data     = test_set_fair, 
                      outcome      = 'Crime_Solved',
                      outcome_base = '0', 
                      group        = 'Victim_Age',
                      probs        = 'probs_1',
                      base         = "30-45")

eq_odds_age[2]

# "Formula: TP / (TP + FN) - sensitivity"

```
Equalized odds is basically how many true positive cases were correctly identified as a ratio of true positive cases and false negative cases. Equalized odds metrics confirm the proportional parity results. Males and older age groups were at disadvantage when it came to the expected chances of solving their crimes (i.e. crimes where they were victims).



# **6. SUMMARY**

The results of the analysis indicate that indeed a bias was to be observed as to whether a crime was solved or not. To the contrary of my expectation, the results do not provide any evidence of bias against race. In the end, the dataset was reduced to contain five "deep south" states, which were not known for their sympathy for ethnic minorities, especially in the 1980s decade. 

Interestingly, the outcome of the analysis is an evidence of bias towards male and older people. These two social groups in multiple metrics are shown to be at considerable disadvantage when it comes to expected probabilites of solving crimes were these two subgroups were victims.




